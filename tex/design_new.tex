\subsection{\techNew{}}

% \subsection{Data Locality in Micrographs}
\label{sec:technew}

%基于
\noindent
\zrdnew{
	\textbf{Observation III: }
%	{Important token indices exhibit strong consistency across adjacent layers of an LLM.} To further analyze the distribution of important token indices, we examine the similarity of the top 25\% important token sets across adjacent layers for different models. As shown in Figure~\ref{fig:ob3}(a), there are two key observations: (1) the important tokens exhibit strong consistency across layers, and (2) tokens with higher importance in one layer are more likely to remain important in the subsequent layer.
The sets of important token indices are highly similar between adjacent transformer layers.}


\ysl{
To understand the nature of inter-layer consistency in token importance, we examine it from two complementary perspectives.  First, we assess the \textit{overlap} of important tokens between adjacent layers. For several OPT models, we select the top 25\% of tokens in each layer based on attention-based importance scores and compute the Jaccard similarity~\cite{jaccard-18} between these sets in neighboring layers. As shown in Figure~\ref{fig:sim_layer}, the similarity consistently falls within a narrow range—e.g., for OPT-6.7B, values span from 0.40 to 0.85 with an average of 0.48—indicating that adjacent layers largely agree on which tokens are important.}

\ysl{Second, we investigate how \textit{relative importance rankings} propagate across layers. Within the top 25\% important tokens of each layer, we partition positions into finer groups by their percentile rank (e.g., the highest group contains the top 20\% most critical tokens in this set). For each group, we measure its recall in the next layer—the fraction of tokens that remain in the top 25\%. As shown in Figure~\ref{fig:layer_wise_r}, tokens with higher importance in the current layer consistently exhibit higher recall, and this trend holds across all models and datasets.}

\ysl{Together, these results reveal that token importance is not only shared across layers but also preserved in rank order—providing a strong foundation for cross-layer prefetching strategies.}

%\zrdnew{To quantify this, we compute the similarity between important token index sets across adjacent Transformer layers for several OPT models. In each layer, we select the top 25\% of tokens ranked by their attention-based importance scores, and measure the similarity between this index set and that of the next layer using the Jaccard index~\cite{jaccard-18}. As shown in Figure~\ref{fig:sim_layer}, each bar represents the similarity between important token sets of  two adjacent layers. We find that similarity values are consistently bounded within a relatively tight interval, with the sampled OPT-6.7B model exhibiting similarities across layers that largely fall between 0.40 and 0.85, and an average similarity of 0.48. This indicates that the attention heads in adjacent layers tend to focus on similar token positions. }
%
%
%\zrdnew{Furthermore, we find that tokens that are more important in one layer tend to remain important in the next layer. As shown in Figure~\ref{fig:layer_wise_r}, we first rank all tokens in each layer by their importance and take the top 25\% as important tokens. Within this important set, we further partition token positions into finer bins according to their relative importance percentiles (e.g., the ``0--20'' bin represents the top 20\% most critical tokens within this set). For each bin, we then measure the recall ratio of important tokens in the next layer, defined as the fraction of tokens in that bin which are still classified as important in the subsequent layer. The resulting curves show that bins containing tokens with higher importance in the current layer consistently achieve larger recall ratios, and all models and datasets exhibit the same overall trend.}



%相邻层的重要token index sets 是相似的。这里需要写一段直觉上有道理的话来支持。我们分析了多个模型的重要token indecs sets相邻层的相似度，when selecting top 25%，每个bar表示本层和下一层重要token index set的相似度。 另外我们还发现，重要性排名越高的token，在下一层中保持重要的概率越大。我们在piqa和openbookqa数据集上，将重要性排名前25%的token当作重要token，
%
\begin{figure}
	\centering
	\includegraphics[width=3.2in, height=0.65in]{sim-25.pdf}
	\vspace{-0.1in}
	\caption{
		\zrdnew{Similarity of important token index sets between adjacent transformer layers in OPT models.}}
	\label{fig:sim_layer}
	\vspace{-0.1in}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=3.3in, height=1.3in]{importance_transition_two_datasets.pdf}
	\vspace{-0.1in}
	\caption{\zrdnew{Recall ratio of next layer important tokens vs.\ current layer importance percentile.}
		}
	\label{fig:layer_wise_r}
	\vspace{-0.1in}
\end{figure}

%如图xx所示，在模型的一层内，识别重要token index set和加载重要token的kv是有数据依赖的，在没有识别得到重要token index set的情况下，我们无法准确预取下一层所需的重要kv，因此在此层计算时，宝贵的I/O带宽没有被利用，在一个本就I/O瓶颈的场景下。一个naive的方法是，随机的预取一些下一层可能用到的token的kvs到GPU memory中，在下一层的重要token选择完成后，再把不在gpu mem的重要kv load到gpu mem中，然而这样的方法受制于完全不知道下一层重要token index的分布，准确率差，浪费了I/O带宽。一些工作提出了attention sink的概念，即一些特定位置的token在推理过程中总是重要的，如prompt的开头和结尾的几个token，然而，仅预取sink位置的token准确率仍不一定高（一些引用），且存在带宽利用不充分的可能。因此我们提出一种准确率高，灵活的指导预取的方法，高效的预取下一层的重要kv，并将预取与计算overlap起来。我们study了\cref{sec:techa}中提到的识别重要token的方法所识别出的重要token，发现在模型的相邻层重要token的分布是相似的，且重要性排名分位越靠前的index集合，相似度越高。相似度仍用jaccard index量化。
%图片xx描述了基于相似性的重要token识别方法识别出的，图片xx

%\zrdnew{For the important tokens identified as shown in Figure xx, within a given layer of the model, there exists a data dependency between identifying the important token index set and loading the corresponding prefix KVs. Without first identifying the important token index set, we are unable to accurately prefetch the necessary KVs for the next layer. As a result, valuable I/O bandwidth remains underutilized during computation in a scenario that is already constrained by I/O bottlenecks. A naive approach would be to randomly prefetch some potential KVs for the next layer into GPU memory. After the important token selection for the next layer is completed, any important KVs not already in GPU memory can then be loaded into it. However, this approach suffers from the challenge of not knowing the distribution of the important token index set for the next layer, resulting in low accuracy and wasted I/O bandwidth. Some works have proposed the concept of attention sinks, where certain tokens at specific positions, such as the beginning and end of a prompt, are always important during inference. However, prefetching only these sink tokens may still result in low accuracy (some references), and there is a risk of inefficient bandwidth utilization.}
%
%\zrdnew{To address these issues, we propose a more accurate and flexible guidance-based prefetching method that efficiently prefetches important KVs for the next layer while overlapping prefetching with computation. We studied the important tokens identified by the method mentioned in \cref{sec:techa} and found that the distribution of important tokens in adjacent layers is highly similar. Moreover, the index sets with higher importance rankings exhibit higher similarity. This similarity is quantified using the Jaccard index.}

%\cp{
%\textbf{Design.} 
%As shown in Figure~\ref{fig:simiload-ttft}(b), KV loading can only start after token selection completes, creating a dependency that causes GPU stalls and limits computation–I/O overlap.
%To mitigate this overhead, we employ prefetching to overlap data loading with computation. 
%However, a key challenge remains: the important tokens for the next transformer layer cannot be identified until the current layer’s computation completes, making it difficult to determine which KVs to prefetch in advance.}
%
%\cp{To solve this challenge, we propose the \technew{} mechanism, which exploits the strong similarity of important token index sets across adjacent layers. 
%During the computation of the current layer, \technew{} asynchronously prefetches the next layer’s important KVs into GPU memory by leveraging the importance distribution from the current layer. 
%For example, 
%as illustrated in Figure~\ref{fig:simiload-ttft}(b), KV loading in Layer~1 cannot begin until token selection completes, causing GPU stalls. 
%In contrast, Figure~\ref{fig:simiload-ttft}(c) shows that using Layer~0’s importance information to guide the prefetching of Layer~1’s KVs during computation effectively hides I/O latency and reduces idle time. 
%Specifically, the important token positions identified in Layer~0 are used as an approximation of those in Layer~1, allowing the system to prefetch the corresponding key–value pairs of tokens at the same positions in the next layer, making them readily available when computation proceeds.
%}
\noindent
\zrdnew{\textbf{Design.} Based on the above observations, we propose the layer-aware KV prefetching mechanism. The core idea is\textit{ to leverage the high similarity of importance distributions across adjacent layers to speculatively prefetch the next layer's critical KVs using idle PCIe bandwidth during the current layer's computation}. This pipelined execution effectively overlaps I/O with GPU processing, hiding data loading latency and eliminating the GPU stalls caused by read-after-compute dependencies.}

\zrdnew{Figure 10 illustrates the impact of this mechanism on inference timelines. As shown in Figure 10(b), without prefetching, KV loading for Layer 1 must wait for token selection to complete, leaving the GPU idle. In contrast, Figure 10(c) shows that by using Layer 0’s importance information to guide the prefetching for Layer 1, the I/O latency is effectively hidden within the computation time of Layer 0, ensuring that KVs are readily available when the execution transitions to the next layer.}

\zrdnew{Implementing this mechanism requires addressing two key issues. First, using the current layer's important tokens to approximate those of the next layer is inherently speculative. Although similarity is high, inaccuracies are inevitable, necessitating a robust verification and recovery mechanism to ensure correctness without incurring high penalties. Second, prefetching I/O must be carefully scheduled. Since this is an opportunistic optimization utilizing idle PCIe bandwidth parallel to computation, the system must ensure that speculative loads do not interfere with the critical execution path or delay the subsequent demand loading of missing KVs.}

\zrdnew{
To address the first issue, we implement a speculate-then-verify workflow. During the computation of layer $i$, \pname{} speculatively loads KVs for layer $i+1$ into a dedicated prefetch buffer, prioritizing tokens with the highest importance scores. 
Upon transitioning to layer $i+1$, the system performs a lightweight verification step: it executes the similarity-guided identification in \S\ref{sec:techa} to determine the ground-truth important tokens. HyperInfer then calculates the set difference between the ground-truth set and the already prefetched KVs. Only the missing KVs are synchronously loaded on the critical path. Since the volume of these missing KVs is typically much smaller than the full set required for demand loading, this recovery step incurs significantly lower latency. Even in the worst-case scenario where prefetching fails completely, the system simply falls back to standard demand loading (Figure~\ref{fig:simiload-ttft}(b)), ensuring that the performance is bounded.}

%\cp{
%To address the second issue, computation and prefetching I/O must be aligned in time, which requires accurate estimates of their durations. 
%During the prefill stage, all transformer layers incur similar computational cost due to their identical structure. 
%Thus, we use the measured computation time of the first layer as a proxy for the rest, treating it as the available prefetching window.
%For I/O estimation, K/V vectors are stored in fixed-size chunks (e.g., \texttt{chunksize}=64). 
%Given a set of target token IDs, we map each token to its corresponding chunk and identify the storage tier. 
%SSD-resident chunks are \texttt{mmap}-loaded into CPU memory before the required K/Vs are transferred to GPU via PCIe, whereas CPU-resident chunks are directly transferred and GPU-resident chunks are accessed immediately. 
%Profiling shows that chunk-fetch latency is stable within each tier. 
%Thus, we measure the average per-chunk latency on SSD, CPU, and GPU and use these values as the I/O cost model. 
%During prefetching, chunks are loaded in descending order of token importance until the prefetching time window  is filled, ensuring that prefetching aligns closely with computation and effectively hides I/O latency.
%}
\zrdnew{To address the second issue, we implement a contention-aware scheduling strategy that precisely aligns prefetching I/O with computation.
First, to determine the safe prefetching window, we leverage the structural homogeneity of Transformer layers. We employ a runtime heuristic that estimates the current layer's computation time using an exponential moving average (EMA) of the execution times from previous layers. This approach filters out transient system jitter and provides a robust time budget. To ensure safety, we set the prefetching deadline conservatively (e.g., 95\% of the estimated window) to prevent I/O from extending onto the critical path.
Second, to prevent resource contention, we execute prefetching commands via asynchronous CUDA streams. This design isolates the I/O operations on the PCIe bus, which is orthogonal to the HBM bandwidth heavily utilized by the computation kernels. Consequently, speculative prefetching can saturate idle I/O bandwidth without interfering with the critical path execution of the current layer.}

\noindent
\zrdnew{\textbf{Handling Mispredictions.} 
	Since layer-aware prefetching is speculative, a small fraction of the loaded KVs may fall outside the target important token set of the next layer. 
	HyperInfer enforces a strict policy to discard these mispredicted KVs, excluding them from the subsequent attention computation. 
	We adopt this design choice for two reasons. 
	First, it ensures performance determinism. Including mispredicted KVs causes sequence lengths to fluctuate based on prediction accuracy. This leads to unpredictable latency jitter. Discarding them ensures the workload remains stable, which is crucial for meeting Service-Level Agreements (SLAs).
	Second, it prevents overhead from negating prefetching benefits. Using extra tokens is not free as it increases matrix calculation time. This added computation cost could consume the time saved by prefetching. Discarding them ensures we hide I/O latency without slowing down the GPU computation.
	Note that since these KVs were transferred using idle PCIe bandwidth, discarding them incurs zero opportunity cost to the critical path.}




%\cp{A straightforward approach is to prefetch data in descending order of importance scores from the previous layer. However, this strategy introduces two major issues.
%(1) The predicted importance is not always accurate (in Figure~\ref{}). Incorrectly prefetched data from SSD can waste substantial parallel I/O time, so the amount of SSD-prefetched data must be carefully controlled.
%(2) Another challenge arises when not all required data are successfully prefetched before inference, leading to missing inputs during computation and requiring a remedy mechanism.}
%
%\cp{
%To mitigate the first issue, we design a dual-tier prefetching (DTP) mechanism that controls the proportion of data fetched from different storage tiers.
%Let $T_{\text{comp}}$ be the computation window during which data loading overlaps with GPU computation.
%DTP divides this window by a ratio $\delta$: $\delta T_{\text{comp}}$ is allocated for memory loading and $(1-\delta)T_{\text{comp}}$ for SSD loading.
%Within each tier, tokens are sorted by predicted importance and prefetched in descending order until the respective budget is used up.
%This confines SSD reads while maintaining effective overlap between I/O and computation.
%Extensive experiments across various workloads show that $\delta=0.8$ consistently achieves the best inference efficiency, as shown in Figure~\ref{}. 
%Thus, \pname{} uses a fixed $\delta=0.8$ in all experiments, while adaptive tuning is left for future work.
%}
%
%\cp{
%To ensure all required data are available during inference, we introduce a lightweight recovery mechanism. After each prefetching window, the system performs a Head-\(K\) loading operation for the next layer to validate prefetched results and fetch missing important KVs, as shown by the red-bordered KV loading in Figure~\ref{fig:simiload-ttft}(c). Token selection is also performed at each layer to update the importance distribution based on the latest computation, ensuring that subsequent prefetching remains accurate. Additionally, incorrectly prefetched data are retained and reused during inference to avoid unnecessary I/O waste.
%}

