\section{Background and Motivation}
\label{background}

\begin{figure}
	\centering
	\includegraphics[width=3.4in, height=1.6in]{transformer.pdf}
	\vspace{-0.05in}
	\caption{The LLM model structure.}
	\label{fig:llm}
	\vspace{-0.1in}
\end{figure}

\subsection{Architecture and Workflow of Large Language Models}
\label{sec:llm-basic}


%A generative LLM typically consists of an input layer, tens of consecutive transformer layers, and an output layer, as shown in Figure~\ref{fig:llm}. Assume an input sequence with \( l \) tokens denoted as \( S = [t_0, t_1, \dots, t_{l-1}] \) and an LLM with \( n \) transformer layers. This sequence is first transformed into a tensor \( X_{\text{in}} \) with shape \( l \times d \) by the input layer, where \( d \) is the model’s hidden dimension. \( X_{\text{in}} \) then passes through the first transformer layer, resulting in an intermediate output tensor \( X_{\text{out\_0}} \) that maintains the shape \( l \times d \). This \( X_{\text{out\_0}} \) becomes the input for the subsequent transformer layer. The final block's output, \( X_{\text{out\_(n-1)}} \), is passed to the output layer, generating the first new token \( t_l \). 
%Then, the newly generated token is fed back into the input layer to generate the next token. This process repeats until either the maximum token limit is reached or a special end-of-sequence (EOS) token is generated, signaling the end of the LLM inference process. The generation of each token is referred to as an \textit{iteration}. The process of generating the first token is called the \textit{prefill} phase, while the subsequent token generation is known as the \textit{decoding} phase.

\noindent \textbf{Model inference.}
\fvc{A generative large language model (LLM) consists of an input layer, a stack of transformer layers, and an output layer, as illustrated in Figure~\ref{fig:llm}. Given an input sequence \( S = [t_0, t_1, \dots, t_{l-1}] \) of \( l \) tokens, the input layer maps it to a tensor \( X_{\text{in}} \in \mathbb{R}^{l \times d} \), where \( d \) is the hidden dimension. This tensor is then processed sequentially through \( n \) transformer layers. Each layer must complete its computation before passing its output \( X_{\text{out}_i} \in \mathbb{R}^{l \times d} \) to the next layer, forming a strictly layer-by-layer dependency. The final layer output \( X_{\text{out}_{n-1}} \) is then fed into the output layer to generate the next token \( t_l \). The new token is appended to the input sequence and reprocessed to generate subsequent tokens.This iterative process continues until reaching the maximum token limit or a special end-of-sequence (EOS) token. The generation of the first token is referred to as the \textit{prefill} phase, while the generation of subsequent tokens is termed the \textit{decoding} phase.}


%\textbf{KV Cache Used in Decoding Phase. }
%Since each iteration’s input sequence shares many tokens with the previous iteration, some identical intermediate data (i.e., key and value tensors) are redundantly recalculated, leading to wasted computational resources across iterations.
%To reduce computational overhead, the key and value tensors generated by each transformer layer during an iteration are stored (referred to as the \textit{KV cache}) for reuse in the subsequent iteration. Specifically, in the 0th iteration, the keys and values of all tokens in the input sequence \( S \) must be generated. In subsequent iterations, only the single newly generated token from the last iteration is passed into the LLM, so only that token’s KV needs to be calculated. Due to this difference in computation patterns, researchers often refer to the 0th iteration as the \textit{prefill} phase, with all subsequent iterations collectively known as the \textit{decoding} phase.


%Each transformer layer consists of an attention layer and a feed-forward network (FFN) (we omit the description of layer normalization and residual connections for simplicity in Figure~\ref{fig:llm}). 
%During the prefill phase, the input tensor \( X_{\text{in}} \) is passed through three weight matrices, \( W_q \), \( W_k \), and \( W_v \), to generate three 3D transient tensors: query (Q), key (K), and value (V). Each of these tensors consists of multiple heads (e.g., 3 in Figure~\ref{fig:llm}), with each head containing a 2D tensor referred to as \( q \), \( k \), or \( v \). 
%The Q and K tensors are then used to produce attention weights, where each head has one corresponding 2D attention weight matrix. 
%Each value in the attention weights indicates the relevance of one token to another. 
%The attention weights are then multiplied by the V tensor to form the attention output. 
%This output is passed through a feedforward network (FFN), which consists of two linear layers, ultimately producing the output tensor \( X_{\text{out}} \), with the same shape as \( X_{\text{in}} \).
\noindent \textbf{Transformer layer computation.}
\fvc{Each transformer layer mainly consists of an attention layer and a feed-forward network (FFN), as shown in Figure~\ref{fig:llm}. During the prefill phase, the input tensor \( X_{\text{in}} \) is projected by the weight matrices \( W_q \), \( W_k \), and \( W_v \) to produce the query (\( Q \)), key (\( K \)), and value (\( V \)) tensors. Each tensor consists of multiple attention heads (e.g., three in Figure~\ref{fig:llm}), where each head corresponds to a 2D tensor denoted as \( q \), \( k \), and \( v \), respectively.
The attention weights, computed from \( Q \) and \( K \), represent token relevance and are applied to \( V \) to obtain the attention output. This output then passes through a two-layer FFN to produce \( X_{\text{out}} \), which has the same shape as \( X_{\text{in}} \).}



\begin{figure}
	\centering
	\includegraphics[width=3.4in]{pkvloading.pdf}
	\caption{\zrdnew{The TTFTs of various cases. Assume the LLM model consists of three transformer layers, denoted as ``Lx''.}}
	\label{fig:pkvloading}
\end{figure}

\subsection{\zrdnew{Impact of Long Contexts and Prefix Reuse}}
\label{sec:iobottleneck}
\noindent \textbf{Long TTFT due to the use of context-rich prefixes.}
\fvc{Directly using large models for inference may yield suboptimal results. Specifically, when asked about recent events absent from the training data, the model can produce incorrect or misleading responses due to issues~\cite{siren-arxiv23}. To enhance response quality, applications often augment user \textit{queries} with context-rich \textit{prefixes}, forming complete \textit{requests} that are fed into the LLM as input sequences.
For example, Retrieval-Augmented Generation (RAG)~\cite{rag-nips20} retrieves external documents relevant to the user query. Advanced GPT plugins, such as Chameleon~\cite{chameleon-nips23}, embed tool definitions in the system prompt and use few-shot examples to guide complex reasoning. Multi-turn dialogue systems~\cite{attentionstore-atc24} incorporate previous question–answer pairs to better capture user intent, while self-consistency~\cite{selfcons-ase23} improves accuracy by generating multiple responses and aggregating them through voting.}
%Directly using large models for inference can lead to suboptimal results. For instance, when queried about a recent event not included in the model's training data, the model might provide incorrect answers. Additionally, due to issues such as hallucinations, the model's responses might contain inaccurate or misleading information~\cite{siren-arxiv23}. To improve response quality, applications often prepend user \textit{queries} with additional context-rich \textit{prefixes} to form complete \textit{requests}, which are then fed into the LLM as input sequences.
%For example, Retrieval-Augmented Generation (RAG)~\cite{rag-nips20} searches external knowledge bases for documents relevant to the user's query. Advanced GPT plugins, such as Chameleon~\cite{chameleon-nips23}, include tool definitions in the system prompt and use few-shot examples to guide the LLM in performing complex reasoning tasks. Multi-turn dialogue applications~\cite{attentionstore-atc24} add previous question-answer pairs to the user's latest query for better intent understanding, while the self-consistency technique~\cite{selfcons-ase23} generates multiple responses to the same query and uses voting to improve accuracy.

\fvc{Figures \ref{fig:pkvloading}(a) and \ref{fig:pkvloading}(b) show that although context-rich prefixes enhance response quality, they substantially increase the delay before the first token is produced, known as the time to first token (TTFT). For example, Chameleon adds more than 2,600 tokens before each user query \cite{chameleon-prompt}. Given that an average real-world query contains approximately 750 tokens \cite{sharegpt}, this quadruples the input length and increases TTFT by up to nine times for the OPT-30B model. Such latency severely affects user experience in TTFT-sensitive applications such as real-time chatbots and also reduces system throughput, leading to higher operational costs.}
%Figure~\ref{fig:pkvloading}(a) and Figure~\ref{fig:pkvloading}(b) show that while these context-rich prefixes improve the quality of responses, they also significantly increase the time-to-first-token (TTFT), which is the delay before the model generates the first token. For instance, the Chameleon system adds over 2,600 tokens of context before the user's query~\cite{chameleon-prompt}. Given that the average real-world user query is about 750 tokens~\cite{sharegpt}, this increases the request token count by more than 4$\times$, extending the TTFT by 9$\times$ for the OPT-30B model due to the additional computation. This can negatively impact user experience, especially in TTFT-sensitive applications like real-time chatbots. 
%Besides, it also degrades the system's overall throughput and increases the enterprise costs.	
%This paper focuses on reducing TTFT during the prefill phase without altering the decoding phase. Note that shortening TTFT also reduces decoding latency for other requests, as modern systems use continuous batching, where the decoding of existing and newly-arrived requests are processed together after the completion of the prefill of the new requests~\cite{orca-osdi22}.

\noindent \textbf {\zrdnew{Mitigating latency via Prefix KV reuse.}}
Researchers have observed that these prefixes are often partially or completely shared across different requests~\cite{sglang-arxiv23, chunkattention-arxiv24, cachegen-sigcomm24, ragcache-arxiv24, promptcache-mlsys24, attentionstore-atc24}. For example, similar queries might retrieve partially or entirely the same related documents using RAG; the same GPT plugin can be used multiple times, resulting in identical system prompts across requests. 
Recomputing the K and V tensors in Figure~\ref{fig:llm} for the same prefix leads to wasted computational resources and increased TTFT. To optimize TTFT, existing systems store and reuse the K and V tensors of these shared prefixes (referred to as \textit{prefix KV cache} or simply \textit{prefix KV}). Note that the Q tensor of the prefix is not stored, as it is not needed for subsequent computations~\cite{attentionstore-atc24}. When a new request with a repeated prefix arrives, the system asynchronously preloads its prefix KVs into GPU memory, thereby reducing TTFT during the prefill phase of the new request.
\begin{figure}
	\centering
	\includegraphics[width=3.3in, height=1.55in]{pload_bottleneck.pdf}
	\vspace{-0.1in}
	\caption{TTFT breakdown.
				``ReComp'' refers to not reusing the prefix KV. ``QueryComp''  denotes the remaining computation after loading the prefix KV. }
%	\vspace{-0.1in}
	\label{fig:pload-bottleneck}
	\vspace{-0.15in}
\end{figure}

\subsection{\zrdnew{Limitations of Existing Prefix KV Management Systems}}
%\begin{table}[t]
%	\centering
%	\caption{\cp{Comparison of prefix KV storage designs.}}
%	\label{tab:pkv_comparison}
%	\setlength{\tabcolsep}{3pt}  
%	\renewcommand{\arraystretch}{1.15}
%		\begin{tabular}{lccccc}
%			\specialrule{1.2pt}{0pt}{2pt}
%			\textbf{System} & \makecell{\textbf{CPU}\\\textbf{Memory}} & \textbf{Disk} & \makecell{\textbf{Import.}\\\textbf{Aware}} & \textbf{Caching} & \textbf{Prefetching} \\
%			\specialrule{1.2pt}{0pt}{2pt}
%			\textit{PromptCache}~\cite{promptcache-mlsys24} & \cmark & \xmark & \xmark & \cmark & \cmark \\
%			\textit{SGLang}~\cite{sglang-arxiv23}           & \cmark & \xmark & \xmark & \cmark & \cmark \\
%			\textit{RAGCache}~\cite{ragcache-arxiv24}       & \cmark & \xmark & \xmark & \cmark & \cmark \\
%			\textit{ChunkAttention}~\cite{chunkattention-arxiv24} & \cmark & \xmark & \xmark & \xmark & \cmark \\
%			\textit{AttentionStore}~\cite{attentionstore-atc24}   & \cmark & \cmark & \xmark & \cmark & \cmark \\
%			\textit{IMPRESS}~\cite{impress-fast25}          & \cmark & \cmark & \cmark & \cmark & \xmark \\
%			\textit{\pname{} (Ours)}          & \cmark & \cmark & \cmark & \cmark & \cmark \\
%			\specialrule{1.2pt}{0pt}{2pt}
%		\end{tabular}
%\end{table}
\label{sec:limit}
\zrdnew{An effective prefix KV management system must satisfy two key requirements. First, it must provide sufficient storage capacity to hold a large number of prefix KVs. Second, it must ensure low latency for retrieving prefix KVs, as excessive loading delay can become a bottleneck in LLM inference and limit the reduction of TTFT.
To achieve these goals, techniques such as caching, prefetching, and importance-aware retrieval are often employed. However, existing systems fail to satisfy both requirements simultaneously. They typically prioritize either low latency at the expense of storage capacity or provide sufficient capacity but suffer from high I/O latency.}

\noindent
\zrdnew{\noindent \textbf{Memory-Centric Systems: The Capacity Wall.}  
Systems such as PromptCache~\cite{promptcache-mlsys24}, 
SGLang~\cite{sglang-arxiv23}, 
RAGCache~\cite{ragcache-arxiv24}, 
and ChunkAttention~\cite{chunkattention-arxiv24} 
store prefix KVs solely in GPU and/or CPU memory to ensure low retrieval latency, 
as illustrated in Figure~\ref{fig:pkvloading}(c). 
This design achieves low TTFT by leveraging the high bandwidth of GPU and CPU memory for rapid data access.
However, GPU and CPU memory are inherently capacity-limited, 
causing these systems to quickly exhaust available storage in RAG workloads. 
For example, with OPT-30B in FP16, the KV cache is 1.31 MB per token; an 8k-token retrieved prefix consumes 11 GB, and eight concurrent sessions require about 88 GB, which alone exceeds the capacity of a single 80 GB A100 GPU.}

\noindent
\zrdnew{\noindent \textbf{Disk-Based Systems: The Bandwidth Bottleneck.}  
To overcome capacity constraints, 
AttentionStore~\cite{attentionstore-atc24} extends the storage hierarchy to include local disks, 
providing sufficient scalability for large-context inference. 
However, retrieving data from disk incurs high I/O latency. 
To mitigate this, AttentionStore employs layer-wise prefetching to overlap data loading 
with the current layer's computation, as shown in Figure~\ref{fig:pkvloading}(d). 
Despite these prefetching efforts, the system remains severely bottlenecked by the limited I/O bandwidth of SSDs and PCIe.
Consequently, the massive data transfer latency cannot be fully hidden by computation, 
especially under heavy or preemptive workloads. 
Our study shows that loading prefix KVs from SSD leads to a substantial increase in TTFT, with I/O latency accounting for 51\% to 98\% of the total TTFT, as evidenced in Figure~\ref{fig:pload-bottleneck}.}
%As evidenced in Figure~\ref{fig:pload-bottleneck}, 
%loading prefix KVs from SSD leads to a substantial increase in TTFT,  where I/O latency accounts for 51\% to 98\% of the total time.}

\noindent
\zrdnew{\noindent \textbf{Importance-Aware Systems: The Serial Dependency.}  
To address the bandwidth bottleneck, IMPRESS~\cite{impress-fast25} employs importance-informed sparse retrieval, effectively minimizing the I/O volume.
However, this selectivity sacrifices parallelism: identifying important KV pairs for the next layer depends on the attention activations computed at runtime in the current layer, introducing a strict read-after-compute dependency.  This serial dependency invalidates conventional layer-wise prefetching, as the system cannot determine which KV blocks to fetch in advance.
%The identification of important KVs for the next layer relies on the runtime attention activation of the current layer, forming a strict read-after-compute dependency.
%This dependency renders the standard layer-wise prefetching mechanism inapplicable, as the system cannot determine target KVs in advance.
Consequently, I/O operations cannot be overlapped with computation and remain on the critical path, as shown in Figure~\ref{fig:pkvloading}(e), thereby limiting potential TTFT reduction.}









%An effective prefix KV storage system must meet two requirements. First, it needs sufficient storage capacity to hold enough prefix KVs. Second, the latency for prefetching the prefix KV must be low. Otherwise, it will become a bottleneck of the LLM inference, limiting the reduction of TTFT. 
%Currently, no system can simultaneously meet these two requirements across various scenarios.
%Most existing systems store prefix KVs only in GPU and/or CPU
%memory~\cite{promptcache-mlsys24, sglang-arxiv23, ragcache-arxiv24,
%chunkattention-arxiv24}, as shown in Figure~\ref{fig:pkvloading}(c), to reduce
%TTFT. However, the limited space in GPU and CPU memory quickly becomes
%exhausted. Although the latest prefix KV storage system,
%AttentionStore~\cite{attentionstore-atc24}, stores the prefix KV on both CPU
%memory and disk to provide sufficient storage space, it doesn’t fundamentally
%reduce the load time, as the latency from disk cannot be fully hidden by
%computation in some cases as shown in Figure~\ref{fig:pkvloading}(d).  
%Thus, this approach may fail under heavy request loads or in preemptive scheduling scenarios, due to the bottleneck of disk I/Os.
%\zrdnew{IMPRESS~\cite{impress-fast25} proposes that loading only important KVs to reduce I/O latency while maintaining comparable accuracy. However, the identification and loading of important KVs for the next layer depend on the computation results of the current layer, which makes it impossible to prefetch them during the current layer's computation. As xxx shows, although this approach reduces the I/O volume, it fails to leverage the opportunity to parallelize I/Os with computation, resulting in the I/O bandwidth being underutilized during computation.}
%Figure~\ref{fig:pload-bottleneck} shows the TTFT breakdown for recalculating
%shared prefixes versus loading and reusing prefix KVs from different storage
%media (i.e., GPU memory, CPU memory, and SSD). We vary the number of input prefix tokens from
%128 to 8k. The ``load''  time represents the I/O latency that cannot be hidden by
%query computation. 
%It shows that loading prefix KVs from GPU or CPU results in shorter TTFT compared to recomputation, but loading from SSD leads to longer TTFT. This is due to the I/O latency from SSD to GPU, which is rarely hidden by query computation and accounts for 51\%-98\% of the total TTFT.
%Consequently, prefix KV loading has become a new bottleneck in model inference, particularly for longer prefixes.



%\subsection{Not All KVs Are Equally Important}
%Recent research~\cite{h2o-nips23, infinigen-osdi24, flexgen-icml23, scissorhands-nips23} indicates that not all tokens' KVs are equally important for maintaining LLM inference accuracy. These methods generate and store the full set of KVs during the prefill phase, then  identify and discard the less important tokens' KVs during decoding by analyzing the attention weights. This approach reduces the computational load during the decoding phase while maintaining comparable LLM inference accuracy.
%
%\zrdnew{
%Inspired by this, we propose \pname{}, an importance-informed caching and prefetching system designed to minimize TTFT.
%It addresses the disk I/O bottleneck by \textit{selectively loading} only critical KVs to reduce data volume.
%Crucially, it further hides latency through \technew{}, a mechanism that speculatively prefetches the next layer's KVs based on current-layer signals, effectively overlapping I/O with computation.
%This combination shifts critical I/O operations off the inference path, significantly improving system performance.
%}
%When
%reusing prefix KVs, we aim to load only important KVs
%%  rather than fetching them
%% all. 
%% \wj{Only important prefix tokens' KVs participate in prefill and decoding
	%% computations, while unimportant tokens' KVs are discarded.}
%for prefill and decoding computations, while unimportant tokens' KVs are discarded.
%Selectively loading the important KVs can
%fundamentally alleviate the disk
%bottleneck, thereby reducing TTFT.

% when reusing prefix KVs 

