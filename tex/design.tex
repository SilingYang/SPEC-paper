\section{HyperInfer Design}
\label{design_mainpart}
\zrdnew{In this section, we present \pname{}, an importance-informed three-tiered prefix KV caching and prefetching system.}
%We first introduce the system overview of SPEC and then elaborate on its three key components.}
First, we describe the overall architecture of \pname{} (\S\ref{sec:overview}). 
\zrdnew{Then, we introduce an I/O-efficient technique to identify important tokens (\S\ref{sec:techa}). 
Furthermore, we propose a prefetching technique to mitigate I/O bottlenecks by overlapping computation and I/Os ({\S \ref{sec:technew}})}.
Finally, we explain how prefix KVs are managed across three storage tiers 
to further reduce the latency when loading them into GPU memory (\S\ref{sec:techb}).

\subsection{Overview}
\label{sec:overview}




%We propose \pname{} to provide large storage capacity for prefix KVs while
%ensuring efficient I/O accesses to reduce TTFT. The system is designed based on
%three principles: (1) Using a minimal number of I/Os to identify the
%important KVs within a prefix, allowing only the essential KVs to be loaded
%during the prefill phase; 
%\cp{(2) Since the I/Os for loading essential KVs lie on the critical path, we introduce data prefetching during model computation to mitigate the I/O overhead.}
%(3) Since loading only important KVs could degrade
%the efficiency of existing storage and caching systems, we optimize the
%three-tiered prefix KV management to improve cache hit
%ratios and I/O efficiency.
\zrdnew{To address the challenges outlined in \S\ref{motiv}, \pname{} is designed based on three core principles: 
(1) employing an I/O-efficient method to identify important KVs, ensuring only essential data is loaded during the prefill phase; 
(2) utilizing latency hiding via prefetching, where the system speculatively loads data for future layers to mitigate I/O overhead on the critical path; and 
(3) optimizing tiered storage management to improve cache hit ratios and I/O efficiency.}

Figure~\ref{fig:overview} presents the overall architecture of \pname{}. 
% It consists of a data plane and two control components.
In the data plane, all prefix KVs are stored on disks in chunks, with some
prefix KVs cached in either CPU memory or GPU memory. The data in the two cache
spaces are exclusive to avoid space wastage. The metadata in the CPU memory is
organized using a radix tree~\cite{sglang-arxiv23}, which facilitates quick
searches for the reusable prefix KVs. The runtime space stores
model parameters and intermediate data needed for GPU inference.
\zrdnew{\pname{} has three control components: important token identification (\S\ref{sec:techa}), layer-aware KV prefetching (\S\ref{sec:technew}), and prefix KV placement (\S\ref{sec:techb}). 
The token identification module identifies important tokens within a chunk by loading only partial keys rather than all of them, reducing the amount of data loaded from disks. 
The prefetching module proactively loads critical KVs for future layers during current computation to hide I/O overhead.
The KV placement module manages the storage and data movement of prefix KVs across disks and the two cache spaces to maximize cache efficiency.}

\begin{figure}
	\centering
	\includegraphics[width=3.3in]{overview.pdf}
	\caption{\zrdnew{Overview of the \pname{} system.}}
	\label{fig:overview}
	\vspace{-0.2in}
\end{figure}


%\noindent \textbf{Dataflow of \pname{}.} 
%\cp{Assume that a request $S=[t^p_0,...,t^p_{m}, t^q_0, ...,t^q_{n}]$ arrives.
%$m$ is the number of tokens in the prefix and $n$ is the number of tokens in the query.
%$t^p$ and $t^q$ denote the prefix and non-prefix tokens, respectively. 
%First, given the request, \pname{} queries the database to find the longest common 
%prefix subsequence from all previous requests~\cite{sglang-arxiv23, chunkattention-arxiv24}. 
%Let's assume that the entire prefix $R=[t^p_0,...,t^p_{m}]$ is found in the database, and its corresponding key--value (KV) pairs are stored across GPU memory, CPU memory, or disks.
%Next, \pname{} identifies the important tokens within $R$, assuming $R_{important}=[t^p_{t}, t^p_{t+1}, ..., t^p_s]$ 
%($ 0 \leq t \leq s \leq m$) is identified (\cref{sec:techa}).
%If the KVs in $R_{important}$ are not in GPU memory, they are loaded from lower memory tiers (CPU or disk) (\cref{sec:techb}).
%During this process, the system also predicts the next set of important KVs and proactively prefetches them to overlap I/Os with computation, thereby further mitigating data access latency (\cref{sec:technew}). 
%Both the important KVs and any incorrectly prefetched KVs are passed to the LLM for computation, whereas other KVs of unimportant tokens in $R$ are discarded and do not participate in subsequent inference. 
%Then, the loaded $R_{\text{important}}$ and the query tokens enter the LLM model, completing the remaining computations in the prefill phase. 
%The decoding phase remains unchanged, following existing systems~\cite{alluneed-nips17}.}
\noindent
\zrdnew{\noindent \textbf{Dataflow of \pname{}.} 
	When an inference request arrives, \pname{} executes the following pipeline to process the prefix and generate the response.
	First, the system queries the radix tree to retrieve the longest cached prefix data (stored in GPU, CPU, or disk) that matches the current request~\cite{sglang-arxiv23, chunkattention-arxiv24}.
	Instead of retrieving all data upfront, \pname{} processes the matched prefix in a layer-wise pipeline.
	For the current layer, it employs the similarity-guided identification method (\S\ref{sec:techa}) to filter out unimportant tokens.
	Crucially, to mask I/O latency, this process executes concurrently with computation: during the GPU computation of the current layer, the layer-aware prefetcher (\S\ref{sec:technew}) speculatively loads the essential KVs for the next layer from lower storage tiers.
	This effectively hides the I/O latency behind the GPU computation.
	Any speculatively loaded KVs that are later verified as unimportant are discarded to save memory.
	After processing the prefix, the system computes the remaining non-prefix tokens and enters the standard decoding phase~\cite{alluneed-nips17}.
	Finally, the newly generated KVs are managed by the KV placement module (\S\ref{sec:techb}) and asynchronously persisted to disk for future reuse.}
%\noindent \textbf{Dataflow of \pname{}.} 
%\zrdnew{Assume that a request $S=[t^p_0, t^p_1, ..., t^p_{m-1}, t^q_0, t^q_1, ..., t^q_{n-1}]$ arrives. 
%$m$ is the number of tokens in the prefix and $n$ is the number of tokens in the query. 
%$t^p$ and $t^q$ denote the prefix and non-prefix tokens, respectively. 
%First, given the request, \pname{} searches the radix tree to find the longest common prefix subsequence from all previous requests~\cite{sglang-arxiv23, chunkattention-arxiv24}. 
%Let's assume the result is $R = [t^p_0, t^p_1, ..., t^p_j]$, whose KVs are stored in GPU memory, CPU memory, or disk. 
%There may also be some prefix tokens $NR = [t^p_{j+1}, ..., t^p_{m-1}]$ that are not in the radix tree, and therefore their KVs do not exist in the system. 
%Next, \pname{} employs the I/O-efficient ITF method to identify the important tokens within $R$, assuming a subset $R_{\text{important}}$ is identified (\S\ref{sec:techa}). 
%If KVs in $R_{\text{important}}$ are not in GPU memory, they are retrieved from lower memory tiers. 
%During this process, \pname{} employs layer-aware prefetching to speculatively load the next layer's KVs, effectively overlapping I/Os with computation (\S\ref{sec:technew}).
%Crucially, upon verification, any KVs that were speculatively prefetched but identified as unimportant are strictly discarded from the runtime buffer to maintain resource efficiency.
%Consequently, only the loaded $R_{\text{important}}$, the tokens $NR$, and the query tokens $[t^q_0, t^q_1, ..., t^q_{n-1}]$ are sent into the LLM model, completing the remaining computations in the prefill phase. 
%Essentially, $R_{\text{important}}$ plus $NR$ becomes the defacto prefix used in the LLM inference. 
%Finally, the newly generated KVs for the prefix tokens in $NR$ are stored on disk, and the prefix tokens in $NR$ are inserted into the radix tree for future reuse. 
%The decoding phase remains unchanged, following existing systems~\cite{alluneed-nips17}.}

%Assume that a request $S=[t^p_0, t^p_1,...,t^p_{m-1}, t^q_0, t^q_1,...,t^q_{n-1}]$ arrives.
%$m$ is the number of tokens in prefix and $n$ is the number of tokens in the query.
%$t^p$ and $t^q$ denote the prefix and non-prefix tokens respectively. 
%First, given the request, \pname{} searches the radix tree to find the longest common 
%prefix subsequence from all previous requests~\cite{sglang-arxiv23, chunkattention-arxiv24}. 
%Let's assume the result is  
%$R=[t^p_0, t^p_1, ..., t^p_j]$, whose KVs are
%stored in GPU memory, CPU memory, or disk.
%There may also be some prefix tokens \( NR = [t^p_{j+1}, ..., t^p_{m-1}] \) that are not in the radix tree, and therefore their KVs do not exist in the system.
%Next, \pname{} employs the I/O-efficient ITF method to identify 
%the important tokens within $R$ , assuming $R_{important}=[t^p_{t}, t^p_{t+1}, ..., t^p_s]$ 
%($ 0 \leq t \leq s \leq j$) is identified.
%If KVs in $R_{important}$ are not in GPU memory, they are loaded from disk or CPU memory. 
%The KVs of unimportant tokens in $R$ are not reused and do not participate in further inference.
%Then, the loaded $R_{important}$, the tokens $NR$, and the tokens $[t^q_0, t^q_1,...,t^q_{n-1}]$   
%are sent into LLM model, completing the remaining computations in the prefill phase.
%Essentially, $R_{important}$ plus $NR$ becomes the defacto prefix used in the LLM inference,
%replacing the set of \{$t^p$\} in $S$.
%Finally, the newly generated KVs for the prefix token in $NR$ are stored on disk, 
%and the prefix tokens in $NR$ are inserted into the radix tree for future reuse by other requests. 
%The decoding phase remains unchanged, following existing systems~\cite{alluneed-nips17}.

% \wj{
\noindent \textbf{Importance metric.} 
In this paper, we use the sum of values
in each column of the attention weight matrix as the token's importance,
following the same method in H2O~\cite{h2o-nips23}. A higher sum indicates greater
token importance. Our system is also compatible with other metrics for measuring
token importance~\cite{scissorhands-nips23, flexgen-icml23, infinigen-osdi24}.
% }

%\pname{} uses a radix-tree~\cite{sglang-arxiv23} 
%to manage the mapping from a token ID to its location.
%When a user request arrives, it searches the radix tree
%to identify its prefixes shared with previous requests. 
%For each token in the prefix, we design different
%dataflows depending on whether the token
%is an important token. 
%(1) If a token is an important token and exists
%in the radix-tree, its prefix KV will be moved
%to GPU if it does not exist in GPU.
%(2) If a token is an important token but does not
%exist in the radix-tree, its prefix KV will be
%generated through the prefill computation and decoding
%(illustrated in Section 2.1). After that, it is moved to GPU.
%(3) If a token is not an important token, \pname{}
%will substitue its KV with the KV of one important token 
%identified by ITF in the subsequent inference to
%reduce the amount of data for loading/generating 
%prefix KVs.

%\wj{
%When a user request arrives, it searches the radix tree 
%to identify its shared prefixes with previous requests. 
%For each token in the prefix, we design different 
%dataflows depending on whether the token is shared.
%(1) If a token is not shared, its prefix KV will be 
%generated through the prefill computation (illustrated in \cref{sec:llm-basic}). 
%Afterward, the token is inserted into the radix tree, 
%and its prefix KV is stored on disk for future reuse by other requests.
%(2) If a token is shared, it will be evaluated by the ITF 
%to determine if it is an important token. 
%If it is important and its prefix KV is not in GPU memory, 
%the prefix KV will be fetched to the GPU for subsequent inference computation. 
%If it is not important, its prefix KV will be not used for current request's computations.
%}


%(1) If the token ID does not exist in the radix-tree,
%indicating no shared prefix is found, \pname{} will
%insert the token into the radix-tree, perform a complete 
%prefill computation, split the computed prefix KVs into fixed chunks, 
%and stores them on disks. The stored prefix KVs are periodically 
%reordered and the relevant metadata are updated. 
%(2) If the token is found in the radix-tree,
%\pname{} will identify the location of important prefix KVs 
%in the multi-tier storage and move them to GPUs for
%further inference.
%(3) If a subset of tokens are found in radix-tree, indicating
%the current request has a partially shared prefix with
%previous requests, the system will first identify important
%KVs shared among the heads for the current query through ITF.
%Then, for the important KVs exist in the storage, they
%will be moved to GPU. For the remaining, they will be generated
%through prefill computation, along with all subsequent decoding phases 
%in GPUs. The prefix KVs including the newly generated KVs will be either 
%cached in GPU or CPU memory, or discarded (with a copy always retained on 
%disks). Their corresponding metadata are updated accordingly.

\input{tex/design1}
\input{tex/design_new}
\input{tex/design2}

